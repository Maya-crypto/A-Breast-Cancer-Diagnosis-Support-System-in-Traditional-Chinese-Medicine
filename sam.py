# -*- coding: utf-8 -*-
"""SAM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GRYOHB0cYkHginLrhySyPIxaylM91VGZ
"""

import cv2
import numpy as np
import torch
import json
import os
from pathlib import Path
from segment_anything import SamPredictor, sam_model_registry

# Charger le modèle SAM
sam_checkpoint = "/home/manel/sam_vit_h_4b8939.pth.1"
model_type = "vit_h"
sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
predictor = SamPredictor(sam)

# Dossiers d'entrée et de sortie
data_dir = "tongue_dataset/train"
output_dir = "segmented_output"
Path(output_dir).mkdir(exist_ok=True)

# Lecture des images et JSON
for image_path in Path(data_dir).glob("*.jpg"):
    base_name = image_path.stem
    json_path = image_path.with_suffix(".json")

    if not json_path.exists():
        print(f"JSON manquant pour {base_name}.")
        continue

    # Charger l'image
    image = cv2.imread(str(image_path))
    if image is None:
        print(f"Problème de lecture de {base_name}.")
        continue

    # Charger les annotations JSON
    with open(json_path, "r") as f:
        data = json.load(f)

    # Extraire le bounding box et convertir en entiers
    #x, y, w, h = map(int, data["annotations"][0]["bbox"])
    # Vérifier si la liste "annotations" contient des données
    if "annotations" in data and len(data["annotations"]) > 0:
        x, y, w, h = map(int, data["annotations"][0]["bbox"])
    else:
        print(f"Aucun bounding box trouvé pour {json_path}, fichier ignoré.")
        continue  # Passe au fichier suivant


    # Envoyer l'image complète à SAM et prédire la segmentation
    predictor.set_image(image)
    masks, _, _ = predictor.predict(
        box=np.array([x, y, x + w, y + h]),
        multimask_output=False
    )

    # Extraire uniquement la langue
    mask = masks[0]
    segmented_tongue = np.zeros_like(image)
    segmented_tongue[mask] = image[mask]

    # Découper uniquement la zone de la langue
    segmented_roi = segmented_tongue[y:y+h, x:x+w]

    # Définition de la taille finale
    final_size = 640

    # Redimensionnement tout en conservant la forme
    h_roi, w_roi = segmented_roi.shape[:2]
    scale = min(final_size / w_roi, final_size / h_roi)  # Échelle pour préserver les proportions

    new_w, new_h = int(w_roi * scale), int(h_roi * scale)
    resized_tongue = cv2.resize(segmented_roi, (new_w, new_h), interpolation=cv2.INTER_LINEAR)

    # Ajouter des bordures noires si nécessaire pour atteindre 640x640
    delta_w = final_size - new_w
    delta_h = final_size - new_h
    top, bottom = delta_h // 2, delta_h - (delta_h // 2)
    left, right = delta_w // 2, delta_w - (delta_w // 2)

    padded_tongue = cv2.copyMakeBorder(
        resized_tongue, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0]
    )

    # Sauvegarder l'image segmentée
    save_path = os.path.join(output_dir, f"{base_name}_segmented.jpg")
    cv2.imwrite(save_path, padded_tongue)

    print(f"Image segmentée et redimensionnée enregistrée : {save_path}")

import cv2
import numpy as np
import torch
import json
import os
from pathlib import Path
from segment_anything import SamPredictor, sam_model_registry

# Charger le modèle SAM
sam_checkpoint = "/home/manel/sam_vit_h_4b8939.pth.1"
model_type = "vit_h"
sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
predictor = SamPredictor(sam)

# Fichier JSON unique
json_file = "train/_annotations.coco.json"  # <-- À modifier si nécessaire

# Dossier d'images
data_dir = "train"
output_dir = "segmented_output"
Path(output_dir).mkdir(exist_ok=True)

# Charger les annotations du JSON
with open(json_file, "r") as f:
    data = json.load(f)

# Vérifier si des annotations existent
if "annotations" not in data or len(data["annotations"]) == 0:
    print("Aucun bounding box trouvé, arrêt du programme.")
    exit()

# Parcourir chaque annotation pour segmenter les images non traitées
for annotation in data["annotations"]:
    image_id = annotation["image_id"]
    bbox = annotation["bbox"]

    # Trouver l'image correspondante
    image_info = next((img for img in data["images"] if img["id"] == image_id), None)

    if not image_info:
        print(f"Image ID {image_id} introuvable dans le fichier JSON.")
        continue

    image_path = os.path.join(data_dir, image_info["file_name"])

    # Vérifier si l'image a déjà été segmentée
    segmented_path = os.path.join(output_dir, f"{image_info['file_name'].split('.')[0]}_segmented.jpg")
    if os.path.exists(segmented_path):
        print(f"Image déjà segmentée, ignorée : {segmented_path}")
        continue  # Passe à la prochaine image

    # Charger l'image
    image = cv2.imread(image_path)
    if image is None:
        print(f"Problème de lecture de {image_path}.")
        continue

    # Extraire le bounding box
    x, y, w, h = map(int, bbox)

    # Envoyer l'image complète à SAM et prédire la segmentation
    predictor.set_image(image)
    masks, _, _ = predictor.predict(
        box=np.array([x, y, x + w, y + h]),
        multimask_output=False
    )

    # Extraire uniquement la langue
    mask = masks[0]
    segmented_tongue = np.zeros_like(image)
    segmented_tongue[mask] = image[mask]

    # Découper uniquement la zone de la langue
    segmented_roi = segmented_tongue[y:y+h, x:x+w]

    # Redimensionnement tout en conservant la forme
    final_size = 640
    h_roi, w_roi = segmented_roi.shape[:2]
    scale = min(final_size / w_roi, final_size / h_roi)

    new_w, new_h = int(w_roi * scale), int(h_roi * scale)
    resized_tongue = cv2.resize(segmented_roi, (new_w, new_h), interpolation=cv2.INTER_LINEAR)

    # Ajouter des bordures noires si nécessaire pour atteindre 640x640
    delta_w = final_size - new_w
    delta_h = final_size - new_h
    top, bottom = delta_h // 2, delta_h - (delta_h // 2)
    left, right = delta_w // 2, delta_w - (delta_w // 2)

    padded_tongue = cv2.copyMakeBorder(
        resized_tongue, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0]
    )

    # Sauvegarder l'image segmentée
    cv2.imwrite(segmented_path, padded_tongue)
    print(f"Nouvelle image segmentée et enregistrée : {segmented_path}")